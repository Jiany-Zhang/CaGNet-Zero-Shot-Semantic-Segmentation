# Copyright (C) 2019 NVIDIA Corporation.  All rights reserved.
# Licensed under the CC BY-NC-SA 4.0 license
# (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).


# data options
  # loader
BATCH_SIZE:
    TRAIN: 4
    TEST: 4
SPLIT:
    TRAIN: train
    TEST: novel
NUM_WORKERS: 8
  # image
IMAGE:
    MEAN:
        R: 122.675
        G: 116.669
        B: 104.008
    SIZE:
        TRAIN: 368
        TEST: 512
WARP_IMAGE: True
  # paths
ROOT: ../coco/
datadir: ../SPNet/data/datasets          # path to dataset
save_path: ./Results                                 # path to save results
  # dataset setting
DATAMODE: LoaderZLS
dataset: cocostuff                                   # dataset for train/val
noSBD_ZS3: False                                     # use voc12SBD with SPNet setting / voc12 with ZS3 setting
  # word embedding
embedding: fastnvec                                  # word embedding method
emb_without_normal: False                            # normalize class embedding map or not


# logger options
display_interval: 1             # how often(it) to display output during training
log_interval: 100               # how often(it) to log the training stats
snapshot: 2000              # how often(it) to save trained models
load_to: 0

# optimization options
ITER_MAX: 40000                 # maximum number of training iterations
ITER_MAX_TRANSFER: 20000        # maximum number of training iterations in transfer learning
ITER_MAX_ST: 10000
interval_step1: 100
interval_step2: 100
first: step1
criticUpdates: 1                # times to update Discriminator in each batch
dis_opt:
  OPTIMIZER: adam
  lr: 0.0005
  lr_transfer: 0.0007
  lr_st: 0.00005
  weight_decay: 0.0005
dis_scheduler:
  lr_policy: lambda
  start_decay_iter: 5000
  step_size: 1000
  gamma: 0.3
back_opt:
  OPTIMIZER: sgd
  lr: 0.0005
  lr_transfer: 0.0
  lr_st: 0.00005
  WEIGHT_DECAY: 0.0005
  MOMENTUM: 0.9
back_scheduler:
  lr_policy: poly
  init_lr: 0.0005 # same as lr
  init_lr_transfer: 0.0 # same as lr_transfer
  lr_decay_iter: 100
  init_lr_st: 0.00005
  power: 0.9
  max_iter: 60000 # same as ITER_MAX
  max_iter_transfer: 20000 # same as ITER_MAX_TRANSFER
  max_iter_st: 10000
gen_opt:
  OPTIMIZER: adam
  lr: 0.0002
  lr_transfer: 0.0056
  lr_st: 0.00005
  weight_decay: 0
gen_scheduler:
  lr_policy: lambda
  start_decay_iter: 5000
  step_size: 1000
  gamma: 0.3


# network options
init: kaiming                   # initialization [gaussian/kaiming/xavier/orthogonal]
init_back: /lustre/home/acct-cszlq/cszlq/gzx/coco_contextual/Results/cocostuff/1/models_transfer/newbest.pth
init_dis: /lustre/home/acct-cszlq/cszlq/gzx/coco_contextual/Results/cocostuff/1/models_transfer/newbest.pth
init_gen: /lustre/home/acct-cszlq/cszlq/gzx/coco_contextual/Results/cocostuff/1/models_transfer/newbest.pth
gen_version: 0                  # version of generator network
gen:
  in_dim_mlp: 1200
  out_dim_mlp: 600

dis_version: 0                  # version of discriminator network
dis:
  in_dim_fc: 600
  out_dim_fc: 1024              # output dimension of fc block
  norm_fc: none                 # used normalization method for fc
  activ_fc: relu                # used activation function for fc
  drop_fc: 0.5                  # uesd dropout for fc [none/float]
  out_dim_cls: 183

back_version: 1                 # 0: DeepLabV2_ResNet101_local
back:
  n_classes: 600
  contextual: 1


# loss options
loss_count: 8                   # count of losses
ignore_index: 255               # ignored index when comupting classfication crossEntropyLoss
lambda_D_gp: 10                 # gradient penalty coefficient in wgan mode for Discriminator
lambda_D_cls_real: 0.5          # real-feature classification loss coefficient for Discriminator
lambda_D_cls_fake: 0.5          # fake-feature classification loss coefficient for Discriminator
lambda_D_cls_fake_transfer: 1      # fake-feature classification loss coefficient for Discriminator in transfer learning
lambda_B_KLD: 100                 # KLDiv loss coefficient for Backbone
lambda_B_cls: 0.5                 # classification loss coefficient for Backbone
lambda_G_Content: 50             # content loss coefficient for Generator
lambda_G_cls: 0               # classification loss coefficient for Generator
lambda_G_GAN: 1
lambda_G_cls_transfer: 1        # classification loss coefficient for Generator in transfer learning
content_loss: MMDLoss       # type of content loss [PerceptualLoss/ContentLoss/MMDLoss]
gan_type: lsgan                 # type of Adversarial loss [wgan-gp/lsgan/gan]
num_unseen: 15
gen_unseen_rate: 19
update_back: t 
top_p: 1